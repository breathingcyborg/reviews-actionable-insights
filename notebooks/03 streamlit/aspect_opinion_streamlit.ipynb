{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"aspect_opinion_streamlit.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPYSXiPeqUe/pUnZ5iDOGac"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"T003ohGS-LE9"},"source":["!pip install pyngrok -q\n","!pip install streamlit -q\n","!pip install --upgrade plotly -q"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DgAWbMW6-SIX"},"source":["!python -m spacy download en_core_web_sm\n","!python -m spacy download en_core_web_lg"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"87C5d8Js-b4k","executionInfo":{"status":"ok","timestamp":1638264375956,"user_tz":-330,"elapsed":669,"user":{"displayName":"Breathing Cyborg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02975047009763494626"}},"outputId":"d0e7a34b-33c5-4d7a-a163-0ba15828746a"},"source":["%%writefile preprocessing.py\n","import re\n","\n","def correct_spellings(phrase):\n","  corrections = {\n","    'pricethe': 'price the',\n","    'loudnessand': 'loudness and',\n","    'muffeled': 'muffled',\n","    'expidite': 'expedite',\n","    'suerb': 'superb',\n","    'eeplaces': 'ear pieces',\n","    'exilent': 'excellent',\n","    'worthable': 'worth able',\n","    'soundaverage': 'sound average',\n","    'bukd': 'build',\n","    'breliant': 'brilliant',\n","    'dvsvinyl': 'dvd vinyl',\n","    'qudoes': 'kudos',\n","    'extarnal': 'external',\n","    'heaten': 'heats',\n","    'iseent': 'is not',\n","    'worth the prize': 'worth the price',\n","    \"laptop's\": 'laptop',\n","    \"laptop’s\": 'laptop',\n","    \"aslo\": \"also\",\n","    \"qulity\": \"quality\",\n","    \"qaulity\": \"quality\",\n","    \"sable\": \"cable\" \n","  }\n","  for k, v in corrections.items():\n","    phrase = phrase.replace(f\"{k}\", v)\n","  return phrase\n","\n","def undo_contractions(phrase):\n","    # specific\n","    phrase = re.sub(r\"won[\\'’]t\", \"will not\", phrase)\n","    phrase = re.sub(r\"can[\\'’]t\", \"can not\", phrase)\n","\n","    # general\n","    phrase = re.sub(r\"n[\\'’]t\", \" not\", phrase)\n","    phrase = re.sub(r\"[\\'’]re\", \" are\", phrase)\n","    phrase = re.sub(r\"[\\'’]s\", \" is\", phrase)\n","    phrase = re.sub(r\"[\\'’]d\", \" would\", phrase)\n","    phrase = re.sub(r\"[\\'’]ll\", \" will\", phrase)\n","    phrase = re.sub(r\"[\\'’]t\", \" not\", phrase)\n","    phrase = re.sub(r\"[\\'’]ve\", \" have\", phrase)\n","    phrase = re.sub(r\"[\\'’]m\", \" am\", phrase)\n","    return phrase\n","\n","emoji_regex = re.compile(\"[\"\n","    u\"\\U0001F600-\\U0001F64F\"  # emoticons\n","    u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n","    u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n","    u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n","    u\"\\U00002500-\\U00002BEF\"  # chinese char\n","    u\"\\U00002702-\\U000027B0\"\n","    u\"\\U00002702-\\U000027B0\"\n","    u\"\\U000024C2-\\U0001F251\"\n","    u\"\\U0001f926-\\U0001f937\"\n","    u\"\\U00010000-\\U0010ffff\"\n","    u\"\\u2640-\\u2642\" \n","    u\"\\u2600-\\u2B55\"\n","    u\"\\u200d\"\n","    u\"\\u23cf\"\n","    u\"\\u23e9\"\n","    u\"\\u231a\"\n","    u\"\\ufe0f\"  # dingbats\n","    u\"\\u3030\"\n","                  \"]+\", re.UNICODE)\n","\n","def preprocess_reviews(reviews):\n","  reviews['text'] = reviews['title'] + ' . ' + reviews['review']\n","\n","  reviews['text_cleaned'] = (reviews['text']\n","    .fillna('')\n","    .str.replace(r'([a-z]+)([A-Z])', r'\\1 \\2') # badProduct bad Product\n","    .str.lower()\n","    .str.replace(emoji_regex, '')\n","    .str.replace('\\n', '.')\n","    .str.replace(r'\\s*\\.+\\s*', '. ') # dots and spaces\n","    .str.replace(r'([\\{\\(\\[\\}\\)\\]])', r' \\1 ') # spaces between parenthesis\n","    .str.replace(r'([:])', r' \\1 ') # spaces between :\n","    .str.replace(r'(\\d+\\.?\\d*)', r' \\1 ') # spaces between numbers\n","    .apply(correct_spellings)\n","    .apply(undo_contractions)\n","  )\n","\n","  return reviews"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing preprocessing.py\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eO2JPOAlCfqa","executionInfo":{"status":"ok","timestamp":1638264379160,"user_tz":-330,"elapsed":671,"user":{"displayName":"Breathing Cyborg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02975047009763494626"}},"outputId":"b756d420-3322-49d8-fac8-1aac61bd08b7"},"source":["%%writefile aspects_extraction.py\n","import pandas as pd\n","import numpy as np\n","\n","def has_vectors(doc):\n","  return np.all([token.has_vector for token in doc])\n","\n","def extract_doc_aspects(doc):\n","\n","    prod_pronouns = ['it','this','they','these']\n","\n","    rule1_pairs = []\n","    rule2_pairs = []\n","    rule3_pairs = []\n","    rule4_pairs = []\n","    rule5_pairs = []\n","    rule6_pairs = []\n","    rule7_pairs = []\n","\n","    for token in doc:\n","        if token.text == 'product':\n","          continue\n","\n","        ## FIRST RULE OF DEPENDANCY PARSE -\n","        ## M - Sentiment modifier || A - Aspect\n","        ## RULE = M is child of A with a relationship of amod\n","        A = \"999999\"\n","        M = \"999999\"\n","        if token.dep_ == \"amod\" and not token.is_stop:\n","            M = token.text\n","            A = token.head.text\n","\n","            # add adverbial modifier of adjective (e.g. 'most comfortable headphones')\n","            M_children = token.children\n","            for child_m in M_children:\n","                if(child_m.dep_ == \"advmod\"):\n","                    M_hash = child_m.text\n","                    M = M_hash + \" \" + M\n","                    break\n","\n","            # negation in adjective, the \"no\" keyword is a 'det' of the noun (e.g. no interesting characters)\n","            A_children = token.head.children\n","            for child_a in A_children:\n","                if(child_a.dep_ == \"det\" and child_a.text == 'no'):\n","                    neg_prefix = 'not'\n","                    M = neg_prefix + \" \" + M\n","                    break\n","\n","        if(A != \"999999\" and M != \"999999\"):\n","            if A in prod_pronouns :\n","                A = \"product\"\n","            dict1 = {\"noun\" : A, \"adj\" : M, \"rule\" : 1}\n","            rule1_pairs.append(dict1)\n","\n","\n","        # # SECOND RULE OF DEPENDANCY PARSE -\n","        # # M - Sentiment modifier || A - Aspect\n","        # Direct Object - A is a child of something with relationship of nsubj, while\n","        # M is a child of the same something with relationship of dobj\n","        # Assumption - A verb will have only one NSUBJ and DOBJ\n","        children = token.children\n","        A = \"999999\"\n","        M = \"999999\"\n","        add_neg_pfx = False\n","        for child in children :\n","            if(child.dep_ == \"nsubj\" and not child.is_stop):\n","                A = child.text\n","\n","            if((child.dep_ == \"dobj\" and child.pos_ == \"ADJ\") and not child.is_stop):\n","                M = child.text\n","\n","            if(child.dep_ == \"neg\"):\n","                neg_prefix = child.text\n","                add_neg_pfx = True\n","\n","        if (add_neg_pfx and M != \"999999\"):\n","            M = neg_prefix + \" \" + M\n","\n","        if(A != \"999999\" and M != \"999999\"):\n","            if A in prod_pronouns :\n","                A = \"product\"\n","            dict2 = {\"noun\" : A, \"adj\" : M, \"rule\" : 2}\n","            rule2_pairs.append(dict2)\n","\n","\n","        ## THIRD RULE OF DEPENDANCY PARSE -\n","        ## M - Sentiment modifier || A - Aspect\n","        ## Adjectival Complement - A is a child of something with relationship of nsubj, while\n","        ## M is a child of the same something with relationship of acomp\n","        ## Assumption - A verb will have only one NSUBJ and DOBJ\n","        ## \"The sound of the speakers would be better. The sound of the speakers could be better\" - handled using AUX dependency\n","\n","        children = token.children\n","        A = \"999999\"\n","        M = \"999999\"\n","        add_neg_pfx = False\n","        for child in children :\n","            if(child.dep_ == \"nsubj\" and not child.is_stop):\n","                A = child.text\n","\n","            if(child.dep_ == \"acomp\" and not child.is_stop):\n","                M = child.text\n","\n","            # example - 'this could have been better' -> (this, not better)\n","            if(child.dep_ == \"aux\" and child.tag_ == \"MD\"):\n","                neg_prefix = \"not\"\n","                add_neg_pfx = True\n","\n","            if(child.dep_ == \"neg\"):\n","                neg_prefix = child.text\n","                add_neg_pfx = True\n","\n","        if (add_neg_pfx and M != \"999999\"):\n","            M = neg_prefix + \" \" + M\n","\n","        if(A != \"999999\" and M != \"999999\"):\n","            if A in prod_pronouns :\n","                A = \"product\"\n","            dict3 = {\"noun\" : A, \"adj\" : M, \"rule\" : 3}\n","            rule3_pairs.append(dict3)\n","\n","\n","        ## FOURTH RULE OF DEPENDANCY PARSE -\n","        ## M - Sentiment modifier || A - Aspect\n","\n","        #Adverbial modifier to a passive verb - A is a child of something with relationship of nsubjpass, while\n","        # M is a child of the same something with relationship of advmod\n","\n","        #Assumption - A verb will have only one NSUBJ and DOBJ\n","\n","        children = token.children\n","        A = \"999999\"\n","        M = \"999999\"\n","        add_neg_pfx = False\n","        for child in children :\n","            if((child.dep_ == \"nsubjpass\" or child.dep_ == \"nsubj\") and not child.is_stop):\n","                A = child.text\n","\n","            if(child.dep_ == \"advmod\" and not child.is_stop):\n","                M = child.text\n","                M_children = child.children\n","                for child_m in M_children:\n","                    if(child_m.dep_ == \"advmod\"):\n","                        M_hash = child_m.text\n","                        M = M_hash + \" \" + child.text\n","                        break\n","\n","            if(child.dep_ == \"neg\"):\n","                neg_prefix = child.text\n","                add_neg_pfx = True\n","\n","        if (add_neg_pfx and M != \"999999\"):\n","            M = neg_prefix + \" \" + M\n","\n","        if(A != \"999999\" and M != \"999999\"):\n","            if A in prod_pronouns :\n","                A = \"product\"\n","            dict4 = {\"noun\" : A, \"adj\" : M, \"rule\" : 4}\n","            rule4_pairs.append(dict4)\n","\n","        ## FIFTH RULE OF DEPENDANCY PARSE -\n","        ## M - Sentiment modifier || A - Aspect\n","\n","        #Complement of a copular verb - A is a child of M with relationship of nsubj, while\n","        # M has a child with relationship of cop\n","\n","        #Assumption - A verb will have only one NSUBJ and DOBJ\n","\n","        children = token.children\n","        A = \"999999\"\n","        buf_var = \"999999\"\n","        for child in children :\n","            if(child.dep_ == \"nsubj\" and not child.is_stop):\n","                A = child.text\n","\n","            if(child.dep_ == \"cop\" and not child.is_stop):\n","                buf_var = child.text\n","\n","        if(A != \"999999\" and buf_var != \"999999\"):\n","            if A in prod_pronouns :\n","                A = \"product\"\n","            dict5 = {\"noun\" : A, \"adj\" : token.text, \"rule\" : 5}\n","            rule5_pairs.append(dict5)\n","\n","\n","        ## SIXTH RULE OF DEPENDANCY PARSE -\n","        ## M - Sentiment modifier || A - Aspect\n","        ## Example - \"It ok\", \"ok\" is INTJ (interjections like bravo, great etc)\n","\n","        children = token.children\n","        A = \"999999\"\n","        M = \"999999\"\n","        if(token.pos_ == \"INTJ\" and not token.is_stop):\n","            for child in children :\n","                if(child.dep_ == \"nsubj\" and not child.is_stop):\n","                    A = child.text\n","                    M = token.text\n","\n","        if(A != \"999999\" and M != \"999999\"):\n","            if A in prod_pronouns :\n","                A = \"product\"\n","            dict6 = {\"noun\" : A, \"adj\" : M, \"rule\" : 6}\n","            rule6_pairs.append(dict6)\n","\n","        ## SEVENTH RULE OF DEPENDANCY PARSE -\n","        ## M - Sentiment modifier || A - Aspect\n","        ## ATTR - link between a verb like 'be/seem/appear' and its complement\n","        ## Example: 'this is garbage' -> (this, garbage)\n","\n","        children = token.children\n","        A = \"999999\"\n","        M = \"999999\"\n","        add_neg_pfx = False\n","        for child in children :\n","            if(child.dep_ == \"nsubj\" and not child.is_stop):\n","                A = child.text\n","\n","            if((child.dep_ == \"attr\") and not child.is_stop):\n","                M = child.text\n","\n","            if(child.dep_ == \"neg\"):\n","                neg_prefix = child.text\n","                add_neg_pfx = True\n","\n","        if (add_neg_pfx and M != \"999999\"):\n","            M = neg_prefix + \" \" + M\n","\n","        if(A != \"999999\" and M != \"999999\"):\n","            if A in prod_pronouns :\n","                A = \"product\"\n","            dict7 = {\"noun\" : A, \"adj\" : M, \"rule\" : 7}\n","            rule7_pairs.append(dict7)\n","\n","    aspects = []\n","\n","    aspects = rule1_pairs + rule2_pairs + rule3_pairs +rule4_pairs +rule5_pairs + rule6_pairs + rule7_pairs\n","\n","    return aspects\n","\n","def extract_aspects(nlp, reviews):\n","  aspects = []\n","\n","  data = ([\n","    (x[1], x[0]) for x in reviews['text_cleaned'].reset_index().to_numpy()\n","  ])\n","\n","  for doc, review_id in nlp.pipe(data, as_tuples=True):\n","    doc_aspects = extract_doc_aspects(doc)\n","    doc_aspects = [\n","        [review_id, aspect['noun'], aspect['adj'], aspect['rule']] \n","        for aspect in doc_aspects if not aspect['noun'].lower().startswith('product')\n","    ]\n","    # filter aspects with out of vocubalary nouns\n","    doc_aspects = [\n","        doc_aspect for doc_aspect in doc_aspects \n","        if has_vectors(nlp(doc_aspect[1]))\n","    ]\n","    aspects.extend(doc_aspects)\n","\n","  aspects = pd.DataFrame(aspects, columns=['review_id', 'aspect', 'opinion', 'rule'])\n","\n","  return aspects"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing aspects_extraction.py\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pR8KxQv_LM8O","executionInfo":{"status":"ok","timestamp":1638264396797,"user_tz":-330,"elapsed":627,"user":{"displayName":"Breathing Cyborg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02975047009763494626"}},"outputId":"5bdd8ba3-211f-4f43-d5bc-5b1fd83f5a88"},"source":["%%writefile clustering.py\n","from sklearn.cluster import AgglomerativeClustering\n","import numpy as np\n","\n","def cluster_aspect_terms(nlp, aspects):\n","\n","  aspect_terms = sorted(list(set(aspects['aspect'].values)))\n","\n","  aspect_terms_sizes = aspects.groupby('aspect').size().sort_index().values\n","\n","  aspect_terms_vectors = [doc.vector for doc in nlp.pipe(aspect_terms)]\n","\n","  clusterer = AgglomerativeClustering(n_clusters=None,\n","                                      affinity='cosine',\n","                                      linkage='average',\n","                                      distance_threshold=0.2)\n","\n","  clusterer.fit(aspect_terms_vectors)\n","\n","  term_replacements = {}\n","\n","  for cluster in range(clusterer.n_clusters_):\n","\n","    idxs = np.nonzero(clusterer.labels_ == cluster)[0]\n","\n","    terms = [t for i, t in enumerate(aspect_terms) if i in idxs]\n","\n","    sizes = aspect_terms_sizes[idxs]\n","    \n","    main_term = terms[np.argmax(sizes)]\n","\n","    for term in terms:\n","      term_replacements[term] = main_term\n","\n","  return term_replacements"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Writing clustering.py\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZyjjLaHy_BIf","executionInfo":{"status":"ok","timestamp":1638271358262,"user_tz":-330,"elapsed":60,"user":{"displayName":"Breathing Cyborg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02975047009763494626"}},"outputId":"415f9e94-dd43-4479-abed-955bee0737a1"},"source":["%%writefile app.py\n","import spacy\n","import pandas as pd\n","import streamlit as st\n","from preprocessing import preprocess_reviews\n","from aspects_extraction import extract_aspects\n","from clustering import cluster_aspect_terms\n","import plotly.express as px\n","import matplotlib.pyplot as plt\n","\n","\n","defaultCsv = {\n","    'Serco USB Hub + Sound Card': 'reviews.csv',\n","    'Honey': 'reviews_honey.csv',\n","}\n","\n","st.set_page_config(\n","    page_title=\"Actionble Insights From Reviews\",\n","    layout=\"wide\",\n","\n",")\n","\n","@st.cache\n","def load_reviews(uploaded_file=None, default_file=None):\n","\n","  if default_file is not None:\n","    reviews = pd.read_csv(default_file)\n","\n","  if uploaded_file is not None:\n","    reviews = pd.read_csv(uploaded_file)\n","  \n","  reviews = validate_reviews_dataframe(reviews)\n","\n","  return preprocess_reviews(reviews)\n","\n","def validate_reviews_dataframe(r):\n","  if 'title' not in r.columns:\n","    raise ValueError(\"column title is required\")\n","  if 'review' not in r.columns:\n","    raise ValueError(\"column review is required\")\n","  if 'rating' not in r.columns:\n","    raise ValueError(\"column rating is required\")\n","  if r['title'].dtype != 'O':\n","    raise ValueError(\"column title must be string\")\n","  if r['review'].dtype != 'O':\n","    raise ValueError(\"column review must be string\")\n","  if r['rating'].dtype != 'float64':\n","    raise ValueError(\"column rating must be float\")\n","  r = r.dropna()\n","  if ((r['rating'] < 0) & (r['rating'] > 5)).any():\n","    raise ValueError(\"values in column rating must be between 0 and 5\")\n","  return r\n","\n","@st.cache(allow_output_mutation=True, suppress_st_warning=True)\n","def load_model():\n","  return spacy.load(\"en_core_web_lg\")\n","\n","@st.cache(allow_output_mutation=True, suppress_st_warning=True)\n","def get_aspects(reviews):\n","  nlp = load_model()\n","  return extract_aspects(nlp, reviews)\n","\n","@st.cache(allow_output_mutation=True, suppress_st_warning=True)\n","def cluster_aspects(aspects):\n","  nlp = load_model()\n","  replacements = cluster_aspect_terms(nlp, aspects)\n","  aspects['aspect'] = aspects['aspect'].map(replacements)\n","  return aspects\n","\n","def get_aspects_with_ratings(aspects, reviews):\n","  aspect_with_ratings = pd.merge(aspects,\n","  reviews[['rating']],\n","  left_on='review_id', \n","  right_index=True)\n","  aspect_with_ratings['review_sentiment'] = pd.cut(aspect_with_ratings['rating'], \n","        bins=[0, 3, 4, 5], \n","        right=True,\n","        labels=['Negative', 'Neutral', 'Positive']\n","  )\n","  return aspect_with_ratings\n","\n","def get_aspect_treemap(aspects):\n","  treemap = px.treemap(aspects.groupby(['aspect', 'opinion']).size().reset_index(),\n","      path=[px.Constant('Aspects'), 'aspect', 'opinion'],\n","      values=0,\n","  )\n","  treemap.update_layout(margin = dict(t=0, l=0, r=0, b=0))\n","  return treemap\n","\n","def plot_pain_points(aspect_with_ratings):\n","  pain_points = (aspect_with_ratings\n","    .query('review_sentiment == \"Negative\"')\n","    .groupby('aspect')\n","    .size()\n","    .sort_values(ascending=False)[:10]\n","  )\n","  fig = px.bar(pain_points)\n","  fig.update_layout(margin = dict(t=0, l=0, r=0, b=0))\n","  fig.update_traces(marker_color='red', showlegend=False)\n","  return fig\n","\n","def plot_gain_points(aspect_with_ratings):\n","  gain_points = (aspect_with_ratings\n","    .query('review_sentiment == \"Positive\"')\n","    .groupby('aspect')\n","    .size()\n","    .sort_values(ascending=False)[:10]\n","  )\n","  fig = px.bar(gain_points)\n","  fig.update_layout(margin = dict(t=0, l=0, r=0, b=0))\n","  fig.update_traces(marker_color='green', showlegend=False)\n","  return fig\n","\n","def plot_sentiment_by_aspect(aspect_with_ratings, top=15):\n","  pivot = pd.crosstab(\n","    index=aspect_with_ratings['aspect'],\n","      columns=aspect_with_ratings['review_sentiment'],\n","      margins=True,\n","  ).sort_values(by='All', ascending=False).iloc[1:, :-1]\n","\n","  fig = px.bar(pivot[:top], barmode='group', color_discrete_map={\n","      'Positive': 'green', \n","      'Negative': 'red',\n","      'Neutral': 'blue',\n","  })\n","  fig.update_layout(margin = dict(t=0, l=0, r=0, b=0))\n","  return fig\n","\n","\n","st.write(\"## Actionble Insights From Reviews\")\n","\n","st.write(\"\"\"\n","Key to building a successfull product is understanding what users want and what users don't want.\n","\n","This insight can be useful in serveral ways.\n","\n","1. Designing product that users actually want.\n","2. Fixing defects in product or addressing users pain points.\n","3. Staying ahead of the competition.\n","\n","There are millions of reviews that people leave on sites like amazon, tripadvisor etc. \n","To gain insights from this data, you could either read all the reviews one by one or \n","let machine analyze these reviews and find main topics that user care about.\n","\"\"\")\n","\n","st.write(\"## Extracting Aspect Opinion Pairs\")\n","st.write(\"\"\"\n","Let's say the customer wrote, `The material of the shirt is not soft`. \n","Here `material` is the `aspect` of shirt and `not soft` is the users `opinion`\n","about this aspect. The analyzer finds aspect opinion pairs from the reviews.\n","\"\"\")\n","\n","st.write(\"### Customer Reviews\")\n","st.write(\"\"\"\n","Dataframe containing reviews of the customer. Title, review, and rating columns are required\n","\"\"\")\n","\n","st.sidebar.title(\"Select Reviews File\")\n","\n","default_file = st.sidebar.selectbox(\n","    \"Choose Sample File\", \n","    defaultCsv.keys(),\n",")\n","if default_file is not None:\n","  default_file = defaultCsv[default_file]\n","\n","st.sidebar.write(\"<div style='text-align:center'>or</div>\",  unsafe_allow_html=True)\n","\n","\n","uploaded_file = st.sidebar.file_uploader(\n","    'Choose a CSV File',\n","    type='csv',\n",")\n","st.sidebar.write(\"CSV with title(string), review(string) and ratings(float 0-5) columns\")\n","\n","try:\n","  reviews = load_reviews(uploaded_file, default_file)\n","  st.write(reviews)\n","\n","  aspects = get_aspects(reviews)\n","  aspects = cluster_aspects(aspects)\n","  aspects_with_ratings = get_aspects_with_ratings(aspects, reviews)\n","\n","  st.write(\"### Extracted Aspect Opinion Pairs\")\n","  st.write(\"\"\"\n","  Treemap of aspect opinion pairs extracted from reviews, treemap\n","  is sized according to number of reviews.\n","  \"\"\")\n","  st.plotly_chart(get_aspect_treemap(aspects), use_container_width=True)\n","\n","\n","  st.write(\"### Pain Points And Gain Points\")\n","  col1, col2 = st.columns(2)\n","\n","  with col1:\n","    st.write('Top Pain Points (by number of -ve reviews)')\n","    st.plotly_chart(plot_pain_points(aspects_with_ratings), use_container_width=True)\n","\n","  with col2:\n","    st.write('Top Gain Points (by number of +ve reviews)')\n","    st.plotly_chart(plot_gain_points(aspects_with_ratings), use_container_width=True)\n","\n","  st.write(\"### Sentiment for each aspect\")\n","  st.write('(0-3 Negative) (4 Neutral) (5 Positive)')\n","  st.plotly_chart(plot_sentiment_by_aspect(aspects_with_ratings), use_container_width=True)\n","except ValueError as e:\n","  st.error(e)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting app.py\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2_e0DB3pgIJL","executionInfo":{"status":"ok","timestamp":1638270250879,"user_tz":-330,"elapsed":23,"user":{"displayName":"Breathing Cyborg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02975047009763494626"}},"outputId":"f7505690-0e52-4bcb-9ff4-9f680f63c62d"},"source":["!mkdir .streamlit"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["mkdir: cannot create directory ‘.streamlit’: File exists\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4dw4enE8gB2Y","executionInfo":{"status":"ok","timestamp":1638270251832,"user_tz":-330,"elapsed":17,"user":{"displayName":"Breathing Cyborg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02975047009763494626"}},"outputId":"e88da7ce-1d02-4cba-a609-7b077fa72492"},"source":["%%writefile .streamlit/config.toml\n","[theme]\n","base=\"light\""],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Overwriting .streamlit/config.toml\n"]}]},{"cell_type":"code","metadata":{"id":"1Ey9Y9cR_wKR"},"source":["from pyngrok import ngrok\n","ngrok.kill()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xRrYWmyF_x-e","executionInfo":{"status":"ok","timestamp":1638271365687,"user_tz":-330,"elapsed":133,"user":{"displayName":"Breathing Cyborg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02975047009763494626"}},"outputId":"eda5255b-457b-48eb-aa8e-8db025a2f684"},"source":["public_url = ngrok.connect(8081)\n","print(public_url)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["NgrokTunnel: \"http://96c6-34-86-231-222.ngrok.io\" -> \"http://localhost:8081\"\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"0Rse_ROz_4Ky","executionInfo":{"status":"ok","timestamp":1638271510335,"user_tz":-330,"elapsed":143476,"user":{"displayName":"Breathing Cyborg","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"02975047009763494626"}},"outputId":"43a3ce63-2b49-486f-bc06-088053becc4c"},"source":["!streamlit run app.py --server.port=8081 > /dev/null"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["2021-11-30 11:22:48.722 NumExpr defaulting to 2 threads.\n"]}]}]}